{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e6faad-fed9-480c-b4d2-40547b9c0015",
   "metadata": {},
   "source": [
    "## **<div style=\"text-align:center\"><span style=\"font-size:1em;\"> <code>Segmentez des clients d'un site e-commerce</code></span> </div>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "009a46ec-bd36-492a-aad3-3bf2cdf41f20",
   "metadata": {},
   "source": [
    "- Vous êtes consultant pour **Olist**, une entreprise brésilienne qui propose une solution de vente sur **les marketplaces en ligne**.\n",
    "  \n",
    "- Votre rôle est d’accompagner Olist dans leur projet de monter une équipe Data et leur premier cas d’usage Data Science autour de **la segmentation client.**\n",
    "\n",
    "Ce notebook contient: \n",
    "\n",
    "- =>  l'analyse l'**EDA** (**E**xploratory **D**ata **A**nalysis)\n",
    "\n",
    "- =>  la **création du dataset** OLIST\n",
    "\n",
    "- =>  le **nettoyage du dataset** OLIST\n",
    "\n",
    "- =>  le calcul de la **RFM (Recency - Frequency  - Monetary)** à savoir : 1. **Recence** (**nb. de jour** depuis la dernière cmd) 2. **Frequency** (**Nb. Total de cmd**) 3. **Monetary** (**le montant total des commandes** pour chaque client)\n",
    "\n",
    "- =>  La préparation **(standardisation)* des données** qui nous serviront à entrainer les modèles de Machine Learning, modèles d'apprentissage **non supervisés**\n",
    "\n",
    "_*Transformer les données pour qu'elles aient une moyenne de 0 et un écart-type de 1._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d48ba-a113-4f4a-bb0f-6727ff2ff0c8",
   "metadata": {},
   "source": [
    "# ETAPE #1 - Présentation | Compréhension du jeu de données OLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1014f6d-8442-459b-beae-d4397a77c9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importation des librairies necessaires a - l'Exploratory Data Analysis - (EDA) sur la base de donnees d'OLIST\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Chargement des datasets (les fichiers csv qui constituent le jeu de donnees d'OLIST)\n",
    "datasets = {\n",
    "    \"Geolocalisation\": pd.read_csv('olist_geolocation_dataset.csv'),\n",
    "    \"Customers_Olist\": pd.read_csv('olist_customers_dataset.csv'),\n",
    "    \"Products_Olist\": pd.read_csv('olist_products_dataset.csv'),\n",
    "    \"Items_Olist\": pd.read_csv('olist_order_items_dataset.csv'),\n",
    "    \"Orders_Olist\": pd.read_csv('olist_orders_dataset.csv'),\n",
    "    \"Orders_Payments_Olist\": pd.read_csv('olist_order_payments_dataset.csv'),\n",
    "    \"Orders_Review_Olist\": pd.read_csv('olist_order_reviews_dataset.csv'),\n",
    "    \"Sellers_Olist\": pd.read_csv('olist_sellers_dataset.csv'),\n",
    "    \"Product_Category_Name_Translated_Olist\": pd.read_csv('product_category_name_translation.csv')\n",
    "}\n",
    "\n",
    "# La fonction pour effectuer une analyse exploratoire complete\n",
    "def eda_complet(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Effectue une analyse exploratoire complete.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Le DataFrame a analyser.\n",
    "        dataset_name (str): Le nom du dataset pour le contexte des sorties.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*40}\\nAnalyse du dataset : {dataset_name}\\n{'='*40}\\n\")\n",
    "    \n",
    "    # Apercu des donnees\n",
    "    print(\"Premieres 5 lignes :\\n\", df.head(), \"\\n\")\n",
    "    print(\"Dimensions (lignes, colonnes) :\", df.shape, \"\\n\")\n",
    "    print(\"Informations sur les colonnes :\")\n",
    "    print(df.info(), \"\\n\")\n",
    "    \n",
    "    # Valeurs manquantes\n",
    "    print(\"Valeurs manquantes :\")\n",
    "    valeurs_manquantes = df.isnull().sum()\n",
    "    print(valeurs_manquantes[valeurs_manquantes > 0], \"\\n\")\n",
    "    msno.matrix(df)\n",
    "    plt.title(f\"Valeurs manquantes dans {dataset_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques descriptives\n",
    "    print(\"Statistiques descriptives :\\n\", df.describe(include='all'), \"\\n\")\n",
    "    \n",
    "    # Lignes dupliquees\n",
    "    doublons = df.duplicated().sum()\n",
    "    print(f\"Nombre de lignes dupliquees : {doublons}\\n\")\n",
    "    \n",
    "    # Analyse des colonnes numeriques\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(\"Statistiques descriptives des colonnes numeriques :\\n\", df[numeric_cols].describe().T, \"\\n\")\n",
    "        \n",
    "        # Distributions\n",
    "        for col in numeric_cols:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.histplot(df[col], kde=True, bins=30)\n",
    "            plt.title(f\"Distribution de {col}\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Frequence\")\n",
    "            plt.show()\n",
    "        \n",
    "        # Valeurs aberrantes\n",
    "        z_scores = np.abs(zscore(df[numeric_cols].dropna()))\n",
    "        outliers = (z_scores > 3).sum(axis=0)\n",
    "        print(\"Valeurs aberrantes detectees par colonne :\\n\", outliers, \"\\n\")\n",
    "        \n",
    "        # Heatmap de correlation\n",
    "        if len(numeric_cols) > 1:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(df[numeric_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "            plt.title(f\"Heatmap des correlations : {dataset_name}\")\n",
    "            plt.show()\n",
    "    \n",
    "    # Analyse des colonnes categorielles\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"Colonnes categoricielles detectees : {list(categorical_cols)}\\n\")\n",
    "        for col in categorical_cols:\n",
    "            print(f\"Top valeurs pour {col} :\\n\", df[col].value_counts().head(5), \"\\n\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.countplot(data=df, y=col, order=df[col].value_counts().index[:10], palette=\"viridis\")\n",
    "            plt.title(f\"Distribution de {col}\")\n",
    "            plt.xlabel(\"Frequence\")\n",
    "            plt.ylabel(col)\n",
    "            plt.show()\n",
    "\n",
    "    print(f\" Fin de l'analyse pour {dataset_name}\\n\")\n",
    "\n",
    "# Effectuer l'analyse exploratoire pour chaque dataset\n",
    "for nom, df in datasets.items():\n",
    "    try:\n",
    "        eda_complet(df, nom)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'analyse du dataset {nom} : {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4973a9-fe3a-402f-8464-cba74cb8fc8c",
   "metadata": {},
   "source": [
    "# ETAPE #2 - Transformer les données | Création du dataset pour la segmentation des clients RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2cf5c7f-2558-4476-8db0-b8216d036405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire courant : C:\\Python311\\Scripts\\NOUVEAU P5\n"
     ]
    }
   ],
   "source": [
    "# Connaître l'emplacement du fichier référent à la base de données OLIST\n",
    "import os\n",
    "\n",
    "# Obtenir le répertoire courant\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Afficher le répertoire courant\n",
    "print(f\"Répertoire courant : {current_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ba15fb-4fed-4f57-b50b-1d42e9504f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion réussie à la base de données.\n",
      "\n",
      "Chargement des tables : 'order_items', 'orders', 'products', 'customers' et 'order_reviews'...\n",
      "Les tables sont maintenant chargées avec succès.\n",
      "\n",
      "Suppression des colonnes redondantes (index)\n",
      "\n",
      "Vérification des colonnes des tables :\n",
      "Colonnes de 'order_items' : Index(['order_id', 'order_item_id', 'product_id', 'seller_id',\n",
      "       'shipping_limit_date', 'price', 'freight_value'],\n",
      "      dtype='object')\n",
      "Colonnes de 'orders' : Index(['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp',\n",
      "       'order_approved_at', 'order_delivered_carrier_date',\n",
      "       'order_delivered_customer_date', 'order_estimated_delivery_date'],\n",
      "      dtype='object')\n",
      "Colonnes de 'products' : Index(['product_id', 'product_category_name', 'product_name_lenght',\n",
      "       'product_description_lenght', 'product_photos_qty', 'product_weight_g',\n",
      "       'product_length_cm', 'product_height_cm', 'product_width_cm'],\n",
      "      dtype='object')\n",
      "Colonnes de 'customers' : Index(['customer_id', 'customer_unique_id', 'customer_zip_code_prefix',\n",
      "       'customer_city', 'customer_state'],\n",
      "      dtype='object')\n",
      "Colonnes de 'order_reviews' : Index(['review_id', 'order_id', 'review_score', 'review_comment_title',\n",
      "       'review_comment_message', 'review_creation_date',\n",
      "       'review_answer_timestamp'],\n",
      "      dtype='object')\n",
      "\n",
      "Vérification des correspondances clés :\n",
      "Nombre de 'order_id' uniques dans 'order_items' : 98666\n",
      "Nombre de 'order_id' uniques dans 'orders' : 99441\n",
      "Nombre de 'product_id' uniques dans 'products' : 32951\n",
      "Nombre de 'customer_id' uniques dans 'customers' : 99441\n",
      "\n",
      "Jointure de 'order_items' avec 'orders' sur 'order_id'...\n",
      "Nombre de lignes après la jointure avec 'orders' : 112650\n",
      "\n",
      "Jointure avec 'products' sur 'product_id'...\n",
      "Nombre de lignes après la jointure avec 'products' : 112650\n",
      "\n",
      "Jointure avec 'customers' sur 'customer_id'...\n",
      "Nombre de lignes après la jointure avec 'customers' : 112650\n",
      "\n",
      "Jointure avec 'order_reviews' sur 'order_id' pour ajouter 'review_score'...\n",
      "Nombre de lignes après la jointure avec 'order_reviews' : 113314\n",
      "\n",
      "Colonnes finales dans le dataset obtenu :\n",
      "Index(['order_id', 'order_item_id', 'product_id', 'seller_id',\n",
      "       'shipping_limit_date', 'price', 'freight_value', 'customer_id',\n",
      "       'order_status', 'order_purchase_timestamp', 'order_approved_at',\n",
      "       'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
      "       'order_estimated_delivery_date', 'product_category_name',\n",
      "       'product_name_lenght', 'product_description_lenght',\n",
      "       'product_photos_qty', 'product_weight_g', 'product_length_cm',\n",
      "       'product_height_cm', 'product_width_cm', 'customer_unique_id',\n",
      "       'customer_zip_code_prefix', 'customer_city', 'customer_state',\n",
      "       'review_score'],\n",
      "      dtype='object')\n",
      "\n",
      "Aperçu des données finales :\n",
      "                           order_id  order_item_id  \\\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
      "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
      "2  000229ec398224ef6ca0657da4fc703e              1   \n",
      "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
      "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
      "\n",
      "                         product_id                         seller_id  \\\n",
      "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
      "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
      "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
      "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
      "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
      "\n",
      "   shipping_limit_date   price  freight_value  \\\n",
      "0  2017-09-19 09:45:35   58.90          13.29   \n",
      "1  2017-05-03 11:05:13  239.90          19.93   \n",
      "2  2018-01-18 14:48:30  199.00          17.87   \n",
      "3  2018-08-15 10:10:18   12.99          12.79   \n",
      "4  2017-02-13 13:57:51  199.90          18.14   \n",
      "\n",
      "                        customer_id order_status order_purchase_timestamp  \\\n",
      "0  3ce436f183e68e07877b285a838db11a    delivered      2017-09-13 08:59:02   \n",
      "1  f6dd3ec061db4e3987629fe6b26e5cce    delivered      2017-04-26 10:53:06   \n",
      "2  6489ae5e4333f3693df5ad4372dab6d3    delivered      2018-01-14 14:33:31   \n",
      "3  d4eb9395c8c0431ee92fce09860c5a06    delivered      2018-08-08 10:00:35   \n",
      "4  58dbd0b2d70206bf40e62cd34e84d795    delivered      2017-02-04 13:57:51   \n",
      "\n",
      "   ... product_photos_qty product_weight_g product_length_cm  \\\n",
      "0  ...                4.0            650.0              28.0   \n",
      "1  ...                2.0          30000.0              50.0   \n",
      "2  ...                2.0           3050.0              33.0   \n",
      "3  ...                1.0            200.0              16.0   \n",
      "4  ...                1.0           3750.0              35.0   \n",
      "\n",
      "  product_height_cm product_width_cm                customer_unique_id  \\\n",
      "0               9.0             14.0  871766c5855e863f6eccc05f988b23cb   \n",
      "1              30.0             40.0  eb28e67c4c0b83846050ddfb8a35d051   \n",
      "2              13.0             33.0  3818d81c6709e39d06b2738a8d3a2474   \n",
      "3              10.0             15.0  af861d436cfc08b2c2ddefd0ba074622   \n",
      "4              40.0             30.0  64b576fb70d441e8f1b2d7d446e483c5   \n",
      "\n",
      "   customer_zip_code_prefix          customer_city  customer_state  \\\n",
      "0                     28013  campos dos goytacazes              RJ   \n",
      "1                     15775        santa fe do sul              SP   \n",
      "2                     35661          para de minas              MG   \n",
      "3                     12952                atibaia              SP   \n",
      "4                     13226        varzea paulista              SP   \n",
      "\n",
      "   review_score  \n",
      "0           5.0  \n",
      "1           4.0  \n",
      "2           5.0  \n",
      "3           4.0  \n",
      "4           5.0  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "Le fichier CSV 'jointure_complete_avec_review_score.csv' a été créé avec succès dans le répertoire courant.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Chemin d'accès au fichier contenant la base de données OLIST ext.SQLite\n",
    "chemin_fichier = \"C:/Python311/Scripts/NOUVEAU P5/olist.db\"\n",
    "\n",
    "def jointure_dataset(chemin_fichier):\n",
    "    try:\n",
    "        # Vérification de l'existence du fichier\n",
    "        if not os.path.exists(chemin_fichier):\n",
    "            print(f\"Erreur : Le fichier '{chemin_fichier}' n'existe pas.\")\n",
    "            return\n",
    "\n",
    "        # Connexion à la base de données SQLite\n",
    "        connexion = sqlite3.connect(chemin_fichier)\n",
    "        print(\"Connexion réussie à la base de données.\\n\")\n",
    "\n",
    "        # Étape 1 : Chargement des tables nécessaires et connexion aux tables\n",
    "        print(\n",
    "            \"Chargement des tables : 'order_items', 'orders', 'products', \"\n",
    "            \"'customers' et 'order_reviews'...\"\n",
    "        )\n",
    "        order_items = pd.read_sql_query(\"SELECT * FROM order_items\", connexion)\n",
    "        orders = pd.read_sql_query(\"SELECT * FROM orders\", connexion)\n",
    "        products = pd.read_sql_query(\"SELECT * FROM products\", connexion)\n",
    "        customers = pd.read_sql_query(\"SELECT * FROM customers\", connexion)\n",
    "        order_reviews = pd.read_sql_query(\"SELECT * FROM order_reviews\", connexion)\n",
    "        print(\"Les tables sont maintenant chargées avec succès.\\n\")\n",
    "\n",
    "        # Étape 2 : Suppression des colonnes 'index' si elles existent\n",
    "        print(\"Suppression des colonnes redondantes (index)\")\n",
    "        order_items = order_items.drop(columns=['index'], errors='ignore')\n",
    "        orders = orders.drop(columns=['index'], errors='ignore')\n",
    "        products = products.drop(columns=['index'], errors='ignore')\n",
    "        customers = customers.drop(columns=['index'], errors='ignore')\n",
    "        order_reviews = order_reviews.drop(columns=['index'], errors='ignore')\n",
    "\n",
    "        # Étape 3 : Vérification des colonnes des tables\n",
    "        print(\"\\nVérification des colonnes des tables :\")\n",
    "        print(\"Colonnes de 'order_items' :\", order_items.columns)\n",
    "        print(\"Colonnes de 'orders' :\", orders.columns)\n",
    "        print(\"Colonnes de 'products' :\", products.columns)\n",
    "        print(\"Colonnes de 'customers' :\", customers.columns)\n",
    "        print(\"Colonnes de 'order_reviews' :\", order_reviews.columns)\n",
    "\n",
    "        # Étape 4 : Vérification des correspondances clés entre les tables\n",
    "        print(\"\\nVérification des correspondances clés :\")\n",
    "        print(f\"Nombre de 'order_id' uniques dans 'order_items' : {order_items['order_id'].nunique()}\")\n",
    "        print(f\"Nombre de 'order_id' uniques dans 'orders' : {orders['order_id'].nunique()}\")\n",
    "        print(f\"Nombre de 'product_id' uniques dans 'products' : {products['product_id'].nunique()}\")\n",
    "        print(f\"Nombre de 'customer_id' uniques dans 'customers' : {customers['customer_id'].nunique()}\")\n",
    "\n",
    "        # Étape 5 : Jointure 'order_items' avec 'orders'\n",
    "        print(\"\\nJointure de 'order_items' avec 'orders' sur 'order_id'...\")\n",
    "        order_items_orders = pd.merge(order_items, orders, on='order_id', how='inner')\n",
    "        print(f\"Nombre de lignes après la jointure avec 'orders' : {len(order_items_orders)}\")\n",
    "\n",
    "        # Étape 6 : Jointure avec 'products'\n",
    "        print(\"\\nJointure avec 'products' sur 'product_id'...\")\n",
    "        orders_products = pd.merge(order_items_orders, products, on='product_id', how='inner')\n",
    "        print(f\"Nombre de lignes après la jointure avec 'products' : {len(orders_products)}\")\n",
    "\n",
    "        # Étape 7 : Jointure avec 'customers'\n",
    "        print(\"\\nJointure avec 'customers' sur 'customer_id'...\")\n",
    "        orders_products_customers = pd.merge(orders_products, customers, on='customer_id', how='inner')\n",
    "        print(f\"Nombre de lignes après la jointure avec 'customers' : {len(orders_products_customers)}\")\n",
    "\n",
    "        # Étape 8 : Jointure avec 'order_reviews' pour ajouter 'review_score'\n",
    "        print(\"\\nJointure avec 'order_reviews' sur 'order_id' pour ajouter 'review_score'...\")\n",
    "        dataset_final = pd.merge(\n",
    "            orders_products_customers,\n",
    "            order_reviews[['order_id', 'review_score']],\n",
    "            on='order_id',\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"Nombre de lignes après la jointure avec 'order_reviews' : {len(dataset_final)}\")\n",
    "\n",
    "        # Étape 9 : Affichage des colonnes finales et aperçu du dataset\n",
    "        print(\"\\nColonnes finales dans le dataset obtenu :\")\n",
    "        print(dataset_final.columns)\n",
    "\n",
    "        print(\"\\nAperçu des données finales :\")\n",
    "        print(dataset_final.head())\n",
    "\n",
    "        # Étape 10 : Exportation du dataset final en CSV\n",
    "        output_file = \"jointure_complete_avec_review_score.csv\"\n",
    "        dataset_final.to_csv(output_file, index=False)\n",
    "        print(f\"\\nLe fichier CSV '{output_file}' a été créé avec succès dans le répertoire courant.\")\n",
    "\n",
    "        # Fermeture de la connexion\n",
    "        connexion.close()\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Erreur lors de la lecture de la base de données : {e}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"Une erreur inattendue est survenue : {ex}\")\n",
    "\n",
    "\n",
    "# Appel de la fonction\n",
    "jointure_dataset(chemin_fichier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba75b7dd-6735-4364-9592-b09414e114e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion réussie à la base de données.\n",
      "\n",
      "Chargement des tables : 'order_items', 'orders', 'products', 'customers' et 'order_reviews'...\n",
      "Les tables sont maintenant chargées avec succès.\n",
      "\n",
      "Suppression des colonnes redondantes (index)...\n",
      "\n",
      "Vérification des colonnes des tables :\n",
      "Colonnes de 'order_items' : Index(['order_id', 'order_item_id', 'product_id', 'seller_id',\n",
      "       'shipping_limit_date', 'price', 'freight_value'],\n",
      "      dtype='object')\n",
      "Colonnes de 'orders' : Index(['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp',\n",
      "       'order_approved_at', 'order_delivered_carrier_date',\n",
      "       'order_delivered_customer_date', 'order_estimated_delivery_date'],\n",
      "      dtype='object')\n",
      "Colonnes de 'products' : Index(['product_id', 'product_category_name', 'product_name_lenght',\n",
      "       'product_description_lenght', 'product_photos_qty', 'product_weight_g',\n",
      "       'product_length_cm', 'product_height_cm', 'product_width_cm'],\n",
      "      dtype='object')\n",
      "Colonnes de 'customers' : Index(['customer_id', 'customer_unique_id', 'customer_zip_code_prefix',\n",
      "       'customer_city', 'customer_state'],\n",
      "      dtype='object')\n",
      "Colonnes de 'order_reviews' : Index(['review_id', 'order_id', 'review_score', 'review_comment_title',\n",
      "       'review_comment_message', 'review_creation_date',\n",
      "       'review_answer_timestamp'],\n",
      "      dtype='object')\n",
      "\n",
      "Vérification des correspondances clés :\n",
      "Nombre de 'order_id' uniques dans 'order_items' : 98666\n",
      "Nombre de 'order_id' uniques dans 'orders' : 99441\n",
      "Nombre de 'product_id' uniques dans 'products' : 32951\n",
      "Nombre de 'customer_id' uniques dans 'customers' : 99441\n",
      "\n",
      "Jointure de 'order_items' avec 'orders' sur 'order_id'...\n",
      "Nombre de lignes après la jointure avec 'orders' : 112650\n",
      "\n",
      "Jointure avec 'products' sur 'product_id'...\n",
      "Nombre de lignes après la jointure avec 'products' : 112650\n",
      "\n",
      "Jointure avec 'customers' sur 'customer_id'...\n",
      "Nombre de lignes après la jointure avec 'customers' : 112650\n",
      "\n",
      "Jointure avec 'order_reviews' sur 'order_id' pour ajouter 'review_score'...\n",
      "Nombre de lignes après la jointure avec 'order_reviews' : 113314\n",
      "\n",
      "Colonnes finales dans le dataset obtenu :\n",
      "Index(['order_id', 'order_item_id', 'product_id', 'seller_id',\n",
      "       'shipping_limit_date', 'price', 'freight_value', 'customer_id',\n",
      "       'order_status', 'order_purchase_timestamp', 'order_approved_at',\n",
      "       'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
      "       'order_estimated_delivery_date', 'product_category_name',\n",
      "       'product_name_lenght', 'product_description_lenght',\n",
      "       'product_photos_qty', 'product_weight_g', 'product_length_cm',\n",
      "       'product_height_cm', 'product_width_cm', 'customer_unique_id',\n",
      "       'customer_zip_code_prefix', 'customer_city', 'customer_state',\n",
      "       'review_score'],\n",
      "      dtype='object')\n",
      "\n",
      "Aperçu des données finales :\n",
      "                           order_id  order_item_id  \\\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
      "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
      "2  000229ec398224ef6ca0657da4fc703e              1   \n",
      "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
      "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
      "\n",
      "                         product_id                         seller_id  \\\n",
      "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
      "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
      "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
      "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
      "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
      "\n",
      "   shipping_limit_date   price  freight_value  \\\n",
      "0  2017-09-19 09:45:35   58.90          13.29   \n",
      "1  2017-05-03 11:05:13  239.90          19.93   \n",
      "2  2018-01-18 14:48:30  199.00          17.87   \n",
      "3  2018-08-15 10:10:18   12.99          12.79   \n",
      "4  2017-02-13 13:57:51  199.90          18.14   \n",
      "\n",
      "                        customer_id order_status order_purchase_timestamp  \\\n",
      "0  3ce436f183e68e07877b285a838db11a    delivered      2017-09-13 08:59:02   \n",
      "1  f6dd3ec061db4e3987629fe6b26e5cce    delivered      2017-04-26 10:53:06   \n",
      "2  6489ae5e4333f3693df5ad4372dab6d3    delivered      2018-01-14 14:33:31   \n",
      "3  d4eb9395c8c0431ee92fce09860c5a06    delivered      2018-08-08 10:00:35   \n",
      "4  58dbd0b2d70206bf40e62cd34e84d795    delivered      2017-02-04 13:57:51   \n",
      "\n",
      "   ... product_photos_qty product_weight_g product_length_cm  \\\n",
      "0  ...                4.0            650.0              28.0   \n",
      "1  ...                2.0          30000.0              50.0   \n",
      "2  ...                2.0           3050.0              33.0   \n",
      "3  ...                1.0            200.0              16.0   \n",
      "4  ...                1.0           3750.0              35.0   \n",
      "\n",
      "  product_height_cm product_width_cm                customer_unique_id  \\\n",
      "0               9.0             14.0  871766c5855e863f6eccc05f988b23cb   \n",
      "1              30.0             40.0  eb28e67c4c0b83846050ddfb8a35d051   \n",
      "2              13.0             33.0  3818d81c6709e39d06b2738a8d3a2474   \n",
      "3              10.0             15.0  af861d436cfc08b2c2ddefd0ba074622   \n",
      "4              40.0             30.0  64b576fb70d441e8f1b2d7d446e483c5   \n",
      "\n",
      "   customer_zip_code_prefix          customer_city  customer_state  \\\n",
      "0                     28013  campos dos goytacazes              RJ   \n",
      "1                     15775        santa fe do sul              SP   \n",
      "2                     35661          para de minas              MG   \n",
      "3                     12952                atibaia              SP   \n",
      "4                     13226        varzea paulista              SP   \n",
      "\n",
      "   review_score  \n",
      "0           5.0  \n",
      "1           4.0  \n",
      "2           5.0  \n",
      "3           4.0  \n",
      "4           5.0  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "Le fichier CSV 'jointure_complete_avec_review_score.csv' a été créé avec succès dans le répertoire courant.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Chemin d'accès au fichier contenant la base de données OLIST ext.SQLite\n",
    "chemin_fichier = \"C:/Python311/Scripts/NOUVEAU P5/olist.db\"\n",
    "\n",
    "def jointure_dataset(chemin_fichier):\n",
    "    try:\n",
    "        # Vérification de l'existence du fichier\n",
    "        if not os.path.exists(chemin_fichier):\n",
    "            print(f\"Erreur : Le fichier '{chemin_fichier}' n'existe pas.\")\n",
    "            return\n",
    "\n",
    "        # Connexion à la base de données SQLite\n",
    "        connexion = sqlite3.connect(chemin_fichier)\n",
    "        print(\"Connexion réussie à la base de données.\\n\")\n",
    "\n",
    "        # Étape 1 : Chargement des tables nécessaires et connexion aux tables\n",
    "        print(\n",
    "            \"Chargement des tables : 'order_items', 'orders', 'products', \"\n",
    "            \"'customers' et 'order_reviews'...\"\n",
    "        )\n",
    "        order_items = pd.read_sql_query(\"SELECT * FROM order_items\", connexion)\n",
    "        orders = pd.read_sql_query(\"SELECT * FROM orders\", connexion)\n",
    "        products = pd.read_sql_query(\"SELECT * FROM products\", connexion)\n",
    "        customers = pd.read_sql_query(\"SELECT * FROM customers\", connexion)\n",
    "        order_reviews = pd.read_sql_query(\"SELECT * FROM order_reviews\", connexion)\n",
    "        print(\"Les tables sont maintenant chargées avec succès.\\n\")\n",
    "\n",
    "        # Étape 2 : Suppression des colonnes 'index' si elles existent\n",
    "        print(\"Suppression des colonnes redondantes (index)...\")\n",
    "        order_items = order_items.drop(columns=['index'], errors='ignore')\n",
    "        orders = orders.drop(columns=['index'], errors='ignore')\n",
    "        products = products.drop(columns=['index'], errors='ignore')\n",
    "        customers = customers.drop(columns=['index'], errors='ignore')\n",
    "        order_reviews = order_reviews.drop(columns=['index'], errors='ignore')\n",
    "\n",
    "        # Étape 3 : Vérification des colonnes des tables\n",
    "        print(\"\\nVérification des colonnes des tables :\")\n",
    "        print(\"Colonnes de 'order_items' :\", order_items.columns)\n",
    "        print(\"Colonnes de 'orders' :\", orders.columns)\n",
    "        print(\"Colonnes de 'products' :\", products.columns)\n",
    "        print(\"Colonnes de 'customers' :\", customers.columns)\n",
    "        print(\"Colonnes de 'order_reviews' :\", order_reviews.columns)\n",
    "\n",
    "        # Étape 4 : Vérification des correspondances clés entre les tables\n",
    "        print(\"\\nVérification des correspondances clés :\")\n",
    "        print(f\"Nombre de 'order_id' uniques dans 'order_items' : {order_items['order_id'].nunique()}\")\n",
    "        print(f\"Nombre de 'order_id' uniques dans 'orders' : {orders['order_id'].nunique()}\")\n",
    "        print(f\"Nombre de 'product_id' uniques dans 'products' : {products['product_id'].nunique()}\")\n",
    "        print(f\"Nombre de 'customer_id' uniques dans 'customers' : {customers['customer_id'].nunique()}\")\n",
    "\n",
    "        # Étape 5 : Jointure 'order_items' avec 'orders'\n",
    "        print(\"\\nJointure de 'order_items' avec 'orders' sur 'order_id'...\")\n",
    "        order_items_orders = pd.merge(order_items, orders, on='order_id', how='inner')\n",
    "        print(f\"Nombre de lignes après la jointure avec 'orders' : {len(order_items_orders)}\")\n",
    "\n",
    "        # Étape 6 : Jointure avec 'products'\n",
    "        print(\"\\nJointure avec 'products' sur 'product_id'...\")\n",
    "        orders_products = pd.merge(order_items_orders, products, on='product_id', how='inner')\n",
    "        print(f\"Nombre de lignes après la jointure avec 'products' : {len(orders_products)}\")\n",
    "\n",
    "        # Étape 7 : Jointure avec 'customers'\n",
    "        print(\"\\nJointure avec 'customers' sur 'customer_id'...\")\n",
    "        orders_products_customers = pd.merge(orders_products, customers, on='customer_id', how='inner')\n",
    "        print(f\"Nombre de lignes après la jointure avec 'customers' : {len(orders_products_customers)}\")\n",
    "\n",
    "        # Étape 8 : Jointure avec 'order_reviews' pour ajouter 'review_score'\n",
    "        print(\"\\nJointure avec 'order_reviews' sur 'order_id' pour ajouter 'review_score'...\")\n",
    "        dataset_final = pd.merge(\n",
    "            orders_products_customers,\n",
    "            order_reviews[['order_id', 'review_score']],\n",
    "            on='order_id',\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"Nombre de lignes après la jointure avec 'order_reviews' : {len(dataset_final)}\")\n",
    "\n",
    "        # Étape 9 : Affichage des colonnes finales et aperçu du dataset\n",
    "        print(\"\\nColonnes finales dans le dataset obtenu :\")\n",
    "        print(dataset_final.columns)\n",
    "\n",
    "        print(\"\\nAperçu des données finales :\")\n",
    "        print(dataset_final.head())\n",
    "\n",
    "        # Étape 10 : Exportation du dataset final en CSV\n",
    "        output_file = \"jointure_complete_avec_review_score.csv\"\n",
    "        dataset_final.to_csv(output_file, index=False)\n",
    "        print(f\"\\nLe fichier CSV '{output_file}' a été créé avec succès dans le répertoire courant.\")\n",
    "\n",
    "        # Fermeture de la connexion\n",
    "        connexion.close()\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Erreur lors de la lecture de la base de données : {e}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"Une erreur inattendue est survenue : {ex}\")\n",
    "\n",
    "\n",
    "# Appel de la fonction\n",
    "jointure_dataset(chemin_fichier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e562849-90e0-4b4b-a70c-a839e427da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Nom du fichier généré par le script précédent\n",
    "fichier_csv = \"jointure_complete_avec_review_score.csv\"\n",
    "\n",
    "def eda_sur_dataset(fichier_csv):\n",
    "    try:\n",
    "        # Chargement des données\n",
    "        print(\"\\nChargement du fichier CSV pour l'EDA...\")\n",
    "        df = pd.read_csv(fichier_csv)\n",
    "        print(\"Le dataset est chargé\")\n",
    "        print(f\"Nombre de lignes : {df.shape[0]}, Nombre de colonnes : {df.shape[1]}\")\n",
    "\n",
    "        # Aperçu des premières lignes\n",
    "        print(\"\\nAperçu des premières lignes :\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Informations générales sur le dataset\n",
    "        print(\"\\nInformations générales sur le dataset :\")\n",
    "        print(df.info())\n",
    "\n",
    "        # Résumé statistique\n",
    "        print(\"\\nRésumé statistique des colonnes numériques :\")\n",
    "        print(df.describe())\n",
    "\n",
    "        # Vérification des données manquantes\n",
    "        print(\"\\nAnalyse des données manquantes :\")\n",
    "        missing_values = df.isnull().sum()\n",
    "        print(missing_values[missing_values > 0])\n",
    "\n",
    "        # Distribution des variables numériques\n",
    "        print(\"\\nCréation des visualisations des variables numériques...\")\n",
    "        numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        df[numeric_columns].hist(bins=15, figsize=(15, 10), edgecolor='black')\n",
    "        plt.suptitle(\"Distributions des variables numériques\", fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "        # Analyse de la variable `order_status`\n",
    "        if 'order_status' in df.columns:\n",
    "            print(\"\\nDistribution de la variable 'order_status' :\")\n",
    "            order_status_counts = df['order_status'].value_counts()\n",
    "            print(order_status_counts)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.countplot(data=df, x='order_status', order=order_status_counts.index)\n",
    "            plt.title(\"Distribution de 'order_status'\")\n",
    "            plt.xlabel(\"Statut de la commande\")\n",
    "            plt.ylabel(\"Nombre\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "        # Analyse des relations entre quelques variables clés\n",
    "        if {'price', 'freight_value'}.issubset(df.columns):\n",
    "            print(\"\\nAnalyse de la relation entre 'price' et 'freight_value' :\")\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(data=df, x='price', y='freight_value')\n",
    "            plt.title(\"Relation entre 'price' et 'freight_value'\")\n",
    "            plt.xlabel(\"Prix\")\n",
    "            plt.ylabel(\"Valeur du fret\")\n",
    "            plt.show()\n",
    "\n",
    "        if {'price', 'product_category_name'}.issubset(df.columns):\n",
    "            print(\"\\nAnalyse des prix par catégorie de produit :\")\n",
    "            avg_price_by_category = df.groupby('product_category_name')['price'].mean().sort_values(ascending=False)\n",
    "            print(avg_price_by_category.head())\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            avg_price_by_category.plot(kind='bar')\n",
    "            plt.title(\"Prix moyen par catégorie de produit\")\n",
    "            plt.xlabel(\"Catégorie de produit\")\n",
    "            plt.ylabel(\"Prix moyen\")\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.show()\n",
    "\n",
    "        # Corrélation entre les variables numériques\n",
    "        print(\"\\nAnalyse des corrélations entre les variables numériques :\")\n",
    "        correlation_matrix = df[numeric_columns].corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "        plt.title(\"Matrice de corrélation\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nEDA terminé avec succès.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur : Le fichier '{fichier_csv}' est introuvable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur inattendue est survenue : {e}\")\n",
    "\n",
    "# Appel de la fonction pour effectuer l'EDA\n",
    "eda_sur_dataset(fichier_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fc6f32-a227-436f-95b0-1e29fb219cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du dataset d'OLIST\n",
    "# Le fichier 'jointure_complete_avec_review_score.csv' contient les données jointes et consolidées.\n",
    "\n",
    "OLIST_merged_data = pd.read_csv('jointure_complete_avec_review_score.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3bd47c-81a8-474e-916a-fcc643839772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>product_photos_qty</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>review_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-09-19 09:45:35</td>\n",
       "      <td>58.90</td>\n",
       "      <td>13.29</td>\n",
       "      <td>3ce436f183e68e07877b285a838db11a</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-09-13 08:59:02</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>871766c5855e863f6eccc05f988b23cb</td>\n",
       "      <td>28013</td>\n",
       "      <td>campos dos goytacazes</td>\n",
       "      <td>RJ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>1</td>\n",
       "      <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "      <td>dd7ddc04e1b6c2c614352b383efe2d36</td>\n",
       "      <td>2017-05-03 11:05:13</td>\n",
       "      <td>239.90</td>\n",
       "      <td>19.93</td>\n",
       "      <td>f6dd3ec061db4e3987629fe6b26e5cce</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-04-26 10:53:06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>eb28e67c4c0b83846050ddfb8a35d051</td>\n",
       "      <td>15775</td>\n",
       "      <td>santa fe do sul</td>\n",
       "      <td>SP</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000229ec398224ef6ca0657da4fc703e</td>\n",
       "      <td>1</td>\n",
       "      <td>c777355d18b72b67abbeef9df44fd0fd</td>\n",
       "      <td>5b51032eddd242adc84c38acab88f23d</td>\n",
       "      <td>2018-01-18 14:48:30</td>\n",
       "      <td>199.00</td>\n",
       "      <td>17.87</td>\n",
       "      <td>6489ae5e4333f3693df5ad4372dab6d3</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-01-14 14:33:31</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3818d81c6709e39d06b2738a8d3a2474</td>\n",
       "      <td>35661</td>\n",
       "      <td>para de minas</td>\n",
       "      <td>MG</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00024acbcdf0a6daa1e931b038114c75</td>\n",
       "      <td>1</td>\n",
       "      <td>7634da152a4610f1595efa32f14722fc</td>\n",
       "      <td>9d7a1d34a5052409006425275ba1c2b4</td>\n",
       "      <td>2018-08-15 10:10:18</td>\n",
       "      <td>12.99</td>\n",
       "      <td>12.79</td>\n",
       "      <td>d4eb9395c8c0431ee92fce09860c5a06</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 10:00:35</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>af861d436cfc08b2c2ddefd0ba074622</td>\n",
       "      <td>12952</td>\n",
       "      <td>atibaia</td>\n",
       "      <td>SP</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00042b26cf59d7ce69dfabb4e55b4fd9</td>\n",
       "      <td>1</td>\n",
       "      <td>ac6c3623068f30de03045865e4e10089</td>\n",
       "      <td>df560393f3a51e74553ab94004ba5c87</td>\n",
       "      <td>2017-02-13 13:57:51</td>\n",
       "      <td>199.90</td>\n",
       "      <td>18.14</td>\n",
       "      <td>58dbd0b2d70206bf40e62cd34e84d795</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-02-04 13:57:51</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>64b576fb70d441e8f1b2d7d446e483c5</td>\n",
       "      <td>13226</td>\n",
       "      <td>varzea paulista</td>\n",
       "      <td>SP</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id  order_item_id  \\\n",
       "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
       "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
       "2  000229ec398224ef6ca0657da4fc703e              1   \n",
       "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
       "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
       "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
       "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
       "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
       "\n",
       "   shipping_limit_date   price  freight_value  \\\n",
       "0  2017-09-19 09:45:35   58.90          13.29   \n",
       "1  2017-05-03 11:05:13  239.90          19.93   \n",
       "2  2018-01-18 14:48:30  199.00          17.87   \n",
       "3  2018-08-15 10:10:18   12.99          12.79   \n",
       "4  2017-02-13 13:57:51  199.90          18.14   \n",
       "\n",
       "                        customer_id order_status order_purchase_timestamp  \\\n",
       "0  3ce436f183e68e07877b285a838db11a    delivered      2017-09-13 08:59:02   \n",
       "1  f6dd3ec061db4e3987629fe6b26e5cce    delivered      2017-04-26 10:53:06   \n",
       "2  6489ae5e4333f3693df5ad4372dab6d3    delivered      2018-01-14 14:33:31   \n",
       "3  d4eb9395c8c0431ee92fce09860c5a06    delivered      2018-08-08 10:00:35   \n",
       "4  58dbd0b2d70206bf40e62cd34e84d795    delivered      2017-02-04 13:57:51   \n",
       "\n",
       "   ... product_photos_qty product_weight_g product_length_cm  \\\n",
       "0  ...                4.0            650.0              28.0   \n",
       "1  ...                2.0          30000.0              50.0   \n",
       "2  ...                2.0           3050.0              33.0   \n",
       "3  ...                1.0            200.0              16.0   \n",
       "4  ...                1.0           3750.0              35.0   \n",
       "\n",
       "  product_height_cm product_width_cm                customer_unique_id  \\\n",
       "0               9.0             14.0  871766c5855e863f6eccc05f988b23cb   \n",
       "1              30.0             40.0  eb28e67c4c0b83846050ddfb8a35d051   \n",
       "2              13.0             33.0  3818d81c6709e39d06b2738a8d3a2474   \n",
       "3              10.0             15.0  af861d436cfc08b2c2ddefd0ba074622   \n",
       "4              40.0             30.0  64b576fb70d441e8f1b2d7d446e483c5   \n",
       "\n",
       "   customer_zip_code_prefix          customer_city  customer_state  \\\n",
       "0                     28013  campos dos goytacazes              RJ   \n",
       "1                     15775        santa fe do sul              SP   \n",
       "2                     35661          para de minas              MG   \n",
       "3                     12952                atibaia              SP   \n",
       "4                     13226        varzea paulista              SP   \n",
       "\n",
       "   review_score  \n",
       "0           5.0  \n",
       "1           4.0  \n",
       "2           5.0  \n",
       "3           4.0  \n",
       "4           5.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les premières lignes du dataset\n",
    "# Cette commande permet de visualiser un aperçu des 5 premières lignes du dataset\n",
    "# pour vérifier son contenu et sa structure.\n",
    "\n",
    "OLIST_merged_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c769471-74f3-4b53-b47c-8c5e3b6d764b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>product_name_lenght</th>\n",
       "      <th>product_description_lenght</th>\n",
       "      <th>product_photos_qty</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>review_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>113314.000000</td>\n",
       "      <td>113314.000000</td>\n",
       "      <td>113314.000000</td>\n",
       "      <td>111702.000000</td>\n",
       "      <td>111702.000000</td>\n",
       "      <td>111702.000000</td>\n",
       "      <td>113296.000000</td>\n",
       "      <td>113296.000000</td>\n",
       "      <td>113296.000000</td>\n",
       "      <td>113296.000000</td>\n",
       "      <td>113314.000000</td>\n",
       "      <td>112372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.198528</td>\n",
       "      <td>120.478701</td>\n",
       "      <td>19.979428</td>\n",
       "      <td>48.777560</td>\n",
       "      <td>786.899250</td>\n",
       "      <td>2.206908</td>\n",
       "      <td>2091.915037</td>\n",
       "      <td>30.162495</td>\n",
       "      <td>16.584513</td>\n",
       "      <td>23.003539</td>\n",
       "      <td>35122.306670</td>\n",
       "      <td>4.032473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.707016</td>\n",
       "      <td>183.279678</td>\n",
       "      <td>15.783227</td>\n",
       "      <td>10.024616</td>\n",
       "      <td>651.758866</td>\n",
       "      <td>1.719500</td>\n",
       "      <td>3749.804597</td>\n",
       "      <td>16.151737</td>\n",
       "      <td>13.439206</td>\n",
       "      <td>11.708481</td>\n",
       "      <td>29869.796752</td>\n",
       "      <td>1.387849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1003.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.900000</td>\n",
       "      <td>13.080000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>11310.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>74.900000</td>\n",
       "      <td>16.260000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>24340.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>134.900000</td>\n",
       "      <td>21.150000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>59041.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>6735.000000</td>\n",
       "      <td>409.680000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>3992.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>40425.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>99990.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_item_id          price  freight_value  product_name_lenght  \\\n",
       "count  113314.000000  113314.000000  113314.000000        111702.000000   \n",
       "mean        1.198528     120.478701      19.979428            48.777560   \n",
       "std         0.707016     183.279678      15.783227            10.024616   \n",
       "min         1.000000       0.850000       0.000000             5.000000   \n",
       "25%         1.000000      39.900000      13.080000            42.000000   \n",
       "50%         1.000000      74.900000      16.260000            52.000000   \n",
       "75%         1.000000     134.900000      21.150000            57.000000   \n",
       "max        21.000000    6735.000000     409.680000            76.000000   \n",
       "\n",
       "       product_description_lenght  product_photos_qty  product_weight_g  \\\n",
       "count               111702.000000       111702.000000     113296.000000   \n",
       "mean                   786.899250            2.206908       2091.915037   \n",
       "std                    651.758866            1.719500       3749.804597   \n",
       "min                      4.000000            1.000000          0.000000   \n",
       "25%                    348.000000            1.000000        300.000000   \n",
       "50%                    601.000000            1.000000        700.000000   \n",
       "75%                    985.000000            3.000000       1800.000000   \n",
       "max                   3992.000000           20.000000      40425.000000   \n",
       "\n",
       "       product_length_cm  product_height_cm  product_width_cm  \\\n",
       "count      113296.000000      113296.000000     113296.000000   \n",
       "mean           30.162495          16.584513         23.003539   \n",
       "std            16.151737          13.439206         11.708481   \n",
       "min             7.000000           2.000000          6.000000   \n",
       "25%            18.000000           8.000000         15.000000   \n",
       "50%            25.000000          13.000000         20.000000   \n",
       "75%            38.000000          20.000000         30.000000   \n",
       "max           105.000000         105.000000        118.000000   \n",
       "\n",
       "       customer_zip_code_prefix   review_score  \n",
       "count             113314.000000  112372.000000  \n",
       "mean               35122.306670       4.032473  \n",
       "std                29869.796752       1.387849  \n",
       "min                 1003.000000       1.000000  \n",
       "25%                11310.000000       4.000000  \n",
       "50%                24340.000000       5.000000  \n",
       "75%                59041.500000       5.000000  \n",
       "max                99990.000000       5.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher le résumé statistique du dataset\n",
    "# La méthode describe() génère des statistiques descriptives pour les colonnes numériques du dataset,\n",
    "# telles que la moyenne, l'écart type, les valeurs minimales, maximales, et les percentiles.\n",
    "\n",
    "OLIST_merged_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0f4b19-945e-43d7-9741-363685584390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['order_id', 'order_item_id', 'product_id', 'seller_id',\n",
       "       'shipping_limit_date', 'price', 'freight_value', 'customer_id',\n",
       "       'order_status', 'order_purchase_timestamp', 'order_approved_at',\n",
       "       'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
       "       'order_estimated_delivery_date', 'product_category_name',\n",
       "       'product_name_lenght', 'product_description_lenght',\n",
       "       'product_photos_qty', 'product_weight_g', 'product_length_cm',\n",
       "       'product_height_cm', 'product_width_cm', 'customer_unique_id',\n",
       "       'customer_zip_code_prefix', 'customer_city', 'customer_state',\n",
       "       'review_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affichage des colonnes du dataset\n",
    "# La propriété .columns retourne une liste des noms de colonnes du dataset.\n",
    "# Cela permet de vérifier la structure du dataframe et les noms des champs disponibles.\n",
    "\n",
    "OLIST_merged_data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99602ab2-93e4-4e42-897e-f6a6670bce45",
   "metadata": {},
   "source": [
    "# ETAPE #2 - Transformer les données | Nettoyage du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb14cf8-6a2c-451d-acc4-320ad1f18a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nettoyage du dataset avant de passer au calcul de la RFM (Recency / Frequency / Monetary)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Étape 1 : Aperçu initial du dataset\n",
    "print(\"Aperçu des données avant nettoyage :\")\n",
    "print(OLIST_merged_data.info())  # Affiche les informations générales du dataframe, y compris les valeurs manquantes\n",
    "print(OLIST_merged_data.head())  # Affiche un aperçu des 5 premières lignes du dataframe\n",
    "\n",
    "# Vérification et affichage des valeurs manquantes\n",
    "print(\"\\nValeurs manquantes par colonne :\")\n",
    "print(OLIST_merged_data.isnull().sum())\n",
    "\n",
    "# Étape 2 : Gestion des anomalies\n",
    "# Suppression des lignes avec des prix négatifs ou égaux à 0\n",
    "OLIST_merged_data = OLIST_merged_data[OLIST_merged_data['price'] > 0]\n",
    "\n",
    "# Étape 3 : Vérification et conversion des types de données\n",
    "# Conversion des colonnes contenant des dates en format datetime\n",
    "colonnes_dates = [\n",
    "    'order_purchase_timestamp', 'order_approved_at',\n",
    "    'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "    'order_estimated_delivery_date', 'shipping_limit_date'\n",
    "]\n",
    "\n",
    "for col in colonnes_dates:\n",
    "    OLIST_merged_data[col] = pd.to_datetime(OLIST_merged_data[col], errors='coerce')\n",
    "\n",
    "# Étape 4 : Vérification finale après nettoyage\n",
    "print(\"\\nAperçu des données après nettoyage :\")\n",
    "print(OLIST_merged_data.info())  # Vérifie les modifications dans les types de données et les valeurs manquantes\n",
    "print(OLIST_merged_data.head())  # Affiche les 5 premières lignes du dataframe nettoyé\n",
    "\n",
    "# Vérification des valeurs manquantes après le nettoyage\n",
    "print(\"\\nValeurs manquantes après nettoyage :\")\n",
    "print(OLIST_merged_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c8c2b-b47b-457e-88e9-3b0b46971c20",
   "metadata": {},
   "source": [
    "# ETAPE #2 - Transformer les données | Définition de la R.F.M Recency Frequency Monetary pour le criblage Marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b6225-893b-45f4-9118-a23d57c6df9f",
   "metadata": {},
   "source": [
    "\n",
    "- La segmentation RFM ou méthode RFM est une méthode de segmentation principalement développée à l'origine pour les actions de marketing direct des véadistes* et qui s'applique désormais également aux acteurs du e-commerce et du commerce traditionnel. _*Entreprises spécialisées dans la vente à distance._\n",
    "\n",
    "- La segmentation RFM prend en compte la **Récence (date de la dernière commande)**, la **Fréquence des commandes** et le **Montant (de la dernière commande ou sur une période donnée)** pour établir des segments de clients homogènes.\n",
    "\n",
    "- La segmentation RFM permet de cibler les offres, d'établir des segments basés sur la valeur des clients et de prévenir l'attrition en identifiant des segments à risque.\n",
    "\n",
    "- Les données RFM restent des données très utilisées, mais une \"simple\" segmentation RFM est de plus en plus rare. **Les données RFM sont désormais intégrées dans des procédures de ciblage et de segmentation comportant de plus en plus de variables et de données**. Elles dépassent désormais le cadre du marketing direct et peuvent s'appliquer à la publicité display en utilisant une DMP et des procédures de CRM onboarding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f64cdf-871b-4e15-8cd7-1f9281990eb0",
   "metadata": {},
   "source": [
    "Dans cette étude de cas on définit les variables RFM (Recency, Frequency, Monetary) comme suivant :\n",
    "- **Récence** : Le nombre de jours depuis la dernière commande.\n",
    "- **Fréquence** : Le nombre de commandes passées par le client.\n",
    "- **Montant** : Le total des montants dépensés par le client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfa3d6-7510-405c-a68d-fc15f2e09220",
   "metadata": {},
   "source": [
    "# ETAPE #2 - Transformer les données | **RECENCE** (Nb. de jour TOTAL depuis la dernière commande par Client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd108a9-7894-4284-8ee7-528a5799a841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Conversion explicite de 'order_purchase_timestamp' en type datetime\n",
    "# Cette étape garantit que la colonne est bien interprétée comme une série temporelle.\n",
    "OLIST_merged_data['order_purchase_timestamp'] = pd.to_datetime(\n",
    "    OLIST_merged_data['order_purchase_timestamp']\n",
    ")\n",
    "\n",
    "# Définition d'une date de référence\n",
    "# La date de référence est définie comme la date la plus récente dans les données.\n",
    "reference_date = OLIST_merged_data['order_purchase_timestamp'].max()\n",
    "print(f\"Date de référence pour le calcul de la récence : {reference_date}\")\n",
    "\n",
    "# Calcul de la récence\n",
    "# Pour chaque client unique ('customer_unique_id'), on calcule la récence (nombre de jours)\n",
    "# en soustrayant la dernière date d'achat de la date de référence.\n",
    "OLIST_merged_data['recency'] = OLIST_merged_data.groupby('customer_unique_id')[\n",
    "    'order_purchase_timestamp'\n",
    "].transform(lambda x: (reference_date - x.max()).days)\n",
    "\n",
    "# Aperçu du dataset après ajout de la colonne 'recency'\n",
    "print(\"Aperçu du dataset après ajout de la colonne 'recency' :\")\n",
    "print(OLIST_merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ebac6b-845a-474e-bfcc-4f0a985a5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La moyenne de recency est : 242.04252784298498\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne de la colonne 'recency'\n",
    "# La moyenne est calculée pour évaluer la récence moyenne des clients\n",
    "# (nombre moyen de jours depuis la dernière interaction client).\n",
    "recency_mean = OLIST_merged_data['recency'].mean()\n",
    "\n",
    "# Affichage de la moyenne\n",
    "# La valeur calculée est affichée avec un message explicite.\n",
    "print(f\"La moyenne de recency est : {recency_mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232aa3e-6520-4959-802c-147d638f48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la liste des colonnes du dataframe\n",
    "# Cette commande permet de vérifier la structure du dataframe et de s'assurer\n",
    "# que la colonne calculée 'recency' a bien été ajoutée.\n",
    "print(\"Liste des colonnes du dataframe :\")\n",
    "print(OLIST_merged_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b65e9-9f95-4b13-b647-039096b4746c",
   "metadata": {},
   "source": [
    "# ETAPE #2 - Transformer les données | **FREQUENCE** (Nb. TOTAL de Commande par Client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4b5a1-89a3-4953-94b1-d7c3120f28e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calcul de la fréquence\n",
    "# La fréquence représente le nombre total de commandes passées par chaque client unique.\n",
    "# Cette valeur est calculée en regroupant par 'customer_unique_id' et en comptant le nombre\n",
    "# de 'order_id' pour chaque client. Le résultat est ajouté sous forme de nouvelle colonne.\n",
    "OLIST_merged_data['frequency'] = OLIST_merged_data.groupby('customer_unique_id')[\n",
    "    'order_id'\n",
    "].transform('count')\n",
    "\n",
    "# Aperçu du dataset après ajout de la colonne 'frequency'\n",
    "# Affichage des premières lignes du dataset pour vérifier l'ajout et la cohérence de la colonne.\n",
    "print(\"Aperçu du dataset après ajout de la colonne 'frequency' :\")\n",
    "print(OLIST_merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15011a-db77-443b-9a97-1d8a4c49db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la moyenne de la colonne 'frequency'\n",
    "# La moyenne est calculée pour déterminer le nombre moyen de commandes passées\n",
    "# par client sur l'ensemble du dataset.\n",
    "frequency_mean = OLIST_merged_data['frequency'].mean()\n",
    "\n",
    "# Affichage de la moyenne\n",
    "# La moyenne calculée est affichée avec un message explicatif pour plus de clarté.\n",
    "print(f\"La moyenne de la fréquence est : {frequency_mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149b86a-2f26-43c0-80f0-6a9664c2b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la liste des colonnes du dataframe\n",
    "# Cette commande permet de vérifier si la colonne calculée 'frequency' a bien été ajoutée\n",
    "# au dataframe principal. Elle affiche la liste complète des noms de colonnes.\n",
    "print(\"Liste des colonnes du dataframe :\")\n",
    "print(OLIST_merged_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d238fc-9fe5-484a-a18f-8790d22abc66",
   "metadata": {},
   "source": [
    "# ETAPE #2 - Transformer les données | **MONTANT** (Montant total des commandes passées par client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57cd77-ea73-4cb3-abe9-d25fcc9511e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calcul du montant total des commandes par client\n",
    "# La variable 'monetary' représente le montant total des achats effectués par chaque client unique.\n",
    "# Pour cela, on regroupe les données par 'customer_unique_id' et on calcule la somme des prix ('price')\n",
    "# pour chaque client. Le résultat est ajouté au dataframe principal sous forme d'une nouvelle colonne.\n",
    "OLIST_merged_data['monetary'] = OLIST_merged_data.groupby('customer_unique_id')[\n",
    "    'price'\n",
    "].transform('sum')\n",
    "\n",
    "# Aperçu des données après ajout de la colonne 'monetary'\n",
    "# Affichage des premières lignes du dataset pour vérifier l'ajout et la cohérence de la colonne 'monetary'.\n",
    "print(\"Aperçu du dataset avec la colonne 'monetary' :\")\n",
    "print(OLIST_merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ebde3-9bf2-49bb-9856-7260b8af46c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La moyenne de monetary est : 167.09293555959547\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne de la colonne 'monetary'\n",
    "# La moyenne est calculée pour analyser le montant moyen des achats effectués\n",
    "# par client sur l'ensemble du dataset.\n",
    "monetary_mean = OLIST_merged_data['monetary'].mean()\n",
    "\n",
    "# Affichage de la moyenne\n",
    "# La valeur calculée est affichée \n",
    "print(f\"La moyenne de monetary est : {monetary_mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab36990-6c25-4727-bd72-69259113d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la liste des colonnes du dataframe\n",
    "# Cette commande permet de vérifier si la colonne calculée 'monetary' a bien été ajoutée\n",
    "# au dataframe principal. Elle affiche la liste complète des noms de colonnes disponibles.\n",
    "print(\"Liste des colonnes du dataframe :\")\n",
    "print(OLIST_merged_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a64700-b233-4f24-998b-b132fea4bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le nombre de valeurs nulles par colonne\n",
    "# Cette commande permet de vérifier la quantité de valeurs manquantes dans chaque colonne\n",
    "# du dataframe. Ces informations sont essentielles pour combler ou traiter les valeurs\n",
    "# nulles avant d'implémenter des modèles de machine learning non supervisés.\n",
    "print(\"Nombre de valeurs nulles par colonne :\")\n",
    "print(OLIST_merged_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6296d9-4660-42b4-a711-6e5164ae0ed5",
   "metadata": {},
   "source": [
    "# ETAPE #2 - Transformer les données | Traitement du **review score** | Standardisation des données (mise à l'échelle des données)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b060435-9227-4158-a9d2-96f8a5288ffc",
   "metadata": {},
   "source": [
    "- La standardisation permet de centrer les valeurs autour de 0 avec un écart-type de 1, rendant la variable recency comparable à d'autres variables pour les modèles sensibles aux échelles comme **_K-means_** et **_DBSCAN_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96581fcb-dc4d-4dd1-ab8b-ad0c74cdefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Étape 1 : Remplacement des valeurs nulles dans 'review_score' par la moyenne\n",
    "print(\"\\nRemplacement des valeurs nulles dans 'review_score' par la moyenne...\")\n",
    "\n",
    "# Calcul du nombre total de valeurs et du nombre de valeurs manquantes\n",
    "nombre_total_valeurs = OLIST_merged_data['review_score'].shape[0]\n",
    "nombre_valeurs_manquantes = OLIST_merged_data['review_score'].isnull().sum()\n",
    "\n",
    "# Calcul de la moyenne et remplacement des valeurs manquantes\n",
    "review_score_mean = OLIST_merged_data['review_score'].mean()\n",
    "OLIST_merged_data['review_score'] = OLIST_merged_data['review_score'].fillna(review_score_mean)\n",
    "\n",
    "# Affichage des statistiques\n",
    "print(f\"Nombre total de valeurs dans 'review_score' : {nombre_total_valeurs}\")\n",
    "print(f\"Nombre de valeurs remplacées : {nombre_valeurs_manquantes}\")\n",
    "print(f\"Proportion de valeurs remplacées : {nombre_valeurs_manquantes / nombre_total_valeurs:.2%}\")\n",
    "print(f\"Moyenne utilisée pour 'review_score' : {review_score_mean}\")\n",
    "\n",
    "# Étape 2 : Encodage One-Hot de 'review_score'\n",
    "print(\"\\nEncodage de la colonne 'review_score' avec One-Hot Encoding...\")\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Transformer 'review_score' en variables One-Hot\n",
    "review_score_encoded = one_hot_encoder.fit_transform(OLIST_merged_data[['review_score']])\n",
    "\n",
    "# Récupérer les noms des nouvelles colonnes après encodage\n",
    "review_score_columns = [f\"review_score_{int(cat)}\" for cat in one_hot_encoder.categories_[0]]\n",
    "print(f\"Nouvelles colonnes créées : {review_score_columns}\")\n",
    "\n",
    "# Ajouter les colonnes encodées au DataFrame\n",
    "OLIST_merged_data = OLIST_merged_data.drop(columns=['review_score'])  # Supprimer l'ancienne colonne\n",
    "OLIST_merged_data[review_score_columns] = review_score_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0b2b4-a10e-4f4a-afa8-d7cb3b075ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une copie du DataFrame\n",
    "# Cette étape est utile pour préserver le DataFrame original avant d'effectuer des modifications.\n",
    "OLIST_merged_data_copy = OLIST_merged_data.copy()\n",
    "\n",
    "# Exporter la copie si nécessaire\n",
    "# La copie est exportée en fichier CSV pour des usages futurs ou pour sauvegarder l'état actuel des données.\n",
    "OLIST_merged_data_copy.to_csv(\"OLIST_merged_data_copy.csv\", index=False)\n",
    "\n",
    "# Réaffecter la copie au DataFrame d'origine pour les prochaines opérations\n",
    "# Cela permet d'utiliser la version sauvegardée pour les prochaines transformations ou analyses.\n",
    "OLIST_merged_data = OLIST_merged_data_copy\n",
    "\n",
    "# Vérifier les colonnes après la réaffectation\n",
    "# Affiche la liste des colonnes pour confirmer que le DataFrame conserve sa structure attendue.\n",
    "print(\"Liste des colonnes après réaffectation :\")\n",
    "print(OLIST_merged_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3f767-c912-4004-897f-18b91cb699db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vérification de la distribution de la colonne 'recency'\n",
    "# Création d'un histogramme pour visualiser la répartition des valeurs de récence (en jours)\n",
    "plt.hist(OLIST_merged_data['recency'], bins=30, edgecolor='black')\n",
    "plt.title(\"Distribution de la colonne 'recency'\")\n",
    "plt.xlabel(\"Recency (jours)\")\n",
    "plt.ylabel(\"Nombre d'occurrences\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Résumé statistique de la colonne 'recency'\n",
    "# Fournit des statistiques descriptives comme la moyenne, les quartiles, le min et max.\n",
    "print(\"Résumé statistique de la colonne 'recency' :\")\n",
    "print(OLIST_merged_data['recency'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09654b4e-02fe-4f8b-8f16-7bc3f6fa1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialisation du scaler StandardScaler\n",
    "# Le StandardScaler standardise les données en les centrant sur la moyenne (0)\n",
    "# et en les échelonnant selon l'écart type (1).\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardisation de la colonne 'recency'\n",
    "# La méthode fit_transform ajuste le scaler sur les données et les transforme.\n",
    "OLIST_merged_data['recency'] = scaler.fit_transform(OLIST_merged_data[['recency']])\n",
    "\n",
    "# Vérification des résultats après standardisation (moyenne de 0 et un écart-type de 1.)\n",
    "print(\"\\nRésumé statistique de la colonne 'recency' après normalisation :\")\n",
    "print(OLIST_merged_data['recency'].describe())\n",
    "\n",
    "# Aperçu des premières valeurs standardisées\n",
    "print(\"\\nAperçu des premières lignes de la colonne 'recency' après standardisation :\")\n",
    "print(OLIST_merged_data[['recency']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba266c-4de6-4c24-86cd-34f9efb1bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vérification de la distribution de la colonne 'frequency'\n",
    "# Création d'un histogramme pour visualiser la répartition des valeurs de fréquence\n",
    "# (nombre total de commandes par client unique).\n",
    "plt.hist(OLIST_merged_data['frequency'], bins=30, edgecolor='black')\n",
    "plt.title(\"Distribution de la colonne 'frequency'\")\n",
    "plt.xlabel(\"Frequency (nombre de commandes par client)\")\n",
    "plt.ylabel(\"Count (nombre d'occurrences)\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Résumé statistique de la colonne 'frequency'\n",
    "# Affiche des statistiques descriptives comme la moyenne, les quartiles, le min et max.\n",
    "print(\"Résumé statistique de la colonne 'frequency' :\")\n",
    "print(OLIST_merged_data['frequency'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ca7b9-e512-404b-8166-a3568cedcd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialisation du scaler StandardScaler\n",
    "# Le StandardScaler standardise les données en les centrant sur la moyenne (0)\n",
    "# et en les échelonnant selon l'écart type (1).\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardisation de la colonne 'frequency'\n",
    "# La méthode fit_transform ajuste le scaler sur les données et les transforme en même temps.\n",
    "OLIST_merged_data['frequency'] = scaler.fit_transform(OLIST_merged_data[['frequency']])\n",
    "\n",
    "# Vérification des résultats après standardisation\n",
    "print(\"\\nRésumé statistique de la colonne 'frequency' après standardisation :\")\n",
    "print(OLIST_merged_data['frequency'].describe())\n",
    "\n",
    "# Aperçu des premières valeurs standardisées\n",
    "print(\"\\nAperçu des premières lignes de la colonne 'frequency' après standardisation :\")\n",
    "print(OLIST_merged_data[['frequency']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b74ae-cedc-4799-a4d0-57f0eec4a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vérification de la distribution de la colonne 'monetary'\n",
    "# Création d'un histogramme pour visualiser la répartition des valeurs de la colonne 'monetary'\n",
    "# (montant total des achats par client).\n",
    "plt.hist(OLIST_merged_data['monetary'], bins=30, edgecolor='black')\n",
    "plt.title(\"Distribution de la colonne 'monetary'\")\n",
    "plt.xlabel(\"Monetary (montant total des achats)\")\n",
    "plt.ylabel(\"Count (nombre d'occurrences)\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Résumé statistique de la colonne 'monetary'\n",
    "# Fournit des statistiques descriptives comme la moyenne, les quartiles, le min et max.\n",
    "print(\"Résumé statistique de la colonne 'monetary' :\")\n",
    "print(OLIST_merged_data['monetary'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0fa6f-c0a5-47ce-92d7-a1562fd79321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialisation du StandardScaler\n",
    "# Le StandardScaler standardise les données en les centrant sur la moyenne (0)\n",
    "# et en les échelonnant selon l'écart type (1).\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardisation de la colonne 'monetary'\n",
    "# Transformation des valeurs pour qu'elles soient standardisées.\n",
    "OLIST_merged_data['monetary'] = scaler.fit_transform(OLIST_merged_data[['monetary']])\n",
    "\n",
    "# Vérification des résultats après standardisation\n",
    "print(\"\\nRésumé statistique de la colonne 'monetary' après standardisation :\")\n",
    "print(OLIST_merged_data['monetary'].describe())\n",
    "\n",
    "# Aperçu des premières lignes de la colonne standardisée\n",
    "print(\"\\nAperçu des premières lignes de la colonne 'monetary' après standardisation :\")\n",
    "print(OLIST_merged_data[['monetary']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39207bf5-8b52-4e13-8dbb-7615a64e66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lecture du dataset\n",
    "# Chargement des données préparées pour les modèles de machine learning\n",
    "# depuis un fichier CSV nommé 'dataset_prepared_for_ml.csv'.\n",
    "OLIST_merged_data = pd.read_csv('dataset_prepared_for_ml.csv')\n",
    "\n",
    "# Affichage des premières lignes pour vérification\n",
    "# Cette étape permet de visualiser un aperçu des données pour confirmer\n",
    "# que le chargement s'est effectué correctement.\n",
    "print(\"Aperçu des premières lignes du dataset :\")\n",
    "print(OLIST_merged_data.head())\n",
    "\n",
    "# Affichage des informations sur le dataset\n",
    "# Affiche des détails comme le nombre de colonnes, les types de données,\n",
    "# les valeurs non nulles, et la mémoire utilisée par le dataset.\n",
    "print(\"\\nInformations sur le dataset :\")\n",
    "print(OLIST_merged_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae5f97-e472-482d-bd89-a538c03e2845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Afficher les variances des colonnes 'recency', 'frequency', et 'monetary'\n",
    "# La variance mesure la dispersion des valeurs autour de leur moyenne.\n",
    "# Une variance élevée indique une dispersion importante, tandis qu'une variance faible indique que les valeurs sont proches de la moyenne.\n",
    "\n",
    "print(\"Variance de 'recency' :\", OLIST_merged_data['recency'].var())\n",
    "print(\"Variance de 'frequency' :\", OLIST_merged_data['frequency'].var())\n",
    "print(\"Variance de 'monetary' :\", OLIST_merged_data['monetary'].var())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169a874-3a65-4f51-ae26-efc461fa73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résumés statistiques des colonnes sélectionnées pour les modèles de Machine Learning supervisés\n",
    "# La méthode describe() génère des statistiques descriptives pour les colonnes numériques sélectionnées :\n",
    "# - Moyenne, écart type, valeurs minimales et maximales, quartiles.\n",
    "# Ces statistiques permettent de comprendre la distribution des données pour les colonnes :\n",
    "# 'recency', 'frequency', 'monetary' et les colonnes encodées des scores de review (1 à 5).\n",
    "\n",
    "selected_columns = [\n",
    "    'recency', 'frequency', 'monetary', \n",
    "    'review_score_1', 'review_score_2', \n",
    "    'review_score_3', 'review_score_4', 'review_score_5'\n",
    "]\n",
    "\n",
    "print(\"Résumé statistique des colonnes sélectionnées :\")\n",
    "print(OLIST_merged_data[selected_columns].describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
